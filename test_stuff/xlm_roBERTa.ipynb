{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16906f8eaa04c61b3bbd89aa8832561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\nicol/.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\42f548f32366559214515ec137cdd16002968bf6\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\nicol/.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\42f548f32366559214515ec137cdd16002968bf6\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418031a2f1e14381968a2894d95df4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\nicol/.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\42f548f32366559214515ec137cdd16002968bf6\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForMaskedLM.\n",
      "\n",
      "All the weights of XLMRobertaForMaskedLM were initialized from the model checkpoint at xlm-roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForMaskedLM for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\nicol/.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\42f548f32366559214515ec137cdd16002968bf6\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54635f8d1984adf9c6d81601af593f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fcacc813d0c498bb778cc42bc87310a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file sentencepiece.bpe.model from cache at C:\\Users\\nicol/.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\42f548f32366559214515ec137cdd16002968bf6\\sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\nicol/.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\42f548f32366559214515ec137cdd16002968bf6\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\nicol/.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\42f548f32366559214515ec137cdd16002968bf6\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.644407331943512,\n",
       "  'token': 324,\n",
       "  'token_str': 'mi',\n",
       "  'sequence': 'Puoi dirmi se mi pioverà?'},\n",
       " {'score': 0.15966393053531647,\n",
       "  'token': 1053,\n",
       "  'token_str': 'ti',\n",
       "  'sequence': 'Puoi dirmi se ti pioverà?'},\n",
       " {'score': 0.02193170227110386,\n",
       "  'token': 351,\n",
       "  'token_str': 'non',\n",
       "  'sequence': 'Puoi dirmi se non pioverà?'},\n",
       " {'score': 0.021720724180340767,\n",
       "  'token': 409,\n",
       "  'token_str': 'mai',\n",
       "  'sequence': 'Puoi dirmi se mai pioverà?'},\n",
       " {'score': 0.020507188513875008,\n",
       "  'token': 163,\n",
       "  'token_str': 'me',\n",
       "  'sequence': 'Puoi dirmi se me pioverà?'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='xlm-roberta-base')\n",
    "unmasker(\"Puoi dirmi se <mask> pioverà?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset banking77 (C:/Users/nicol/.cache/huggingface/datasets/banking77/default/1.1.0/ff44c4421d7e70aa810b0fa79d36908a38b87aff8125d002cd44f7fcd31f493c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb1e6b5213f46fd833c8418a2c8513a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 10003\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 3080\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('banking77')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['activate_my_card', 'age_limit', 'apple_pay_or_google_pay', 'atm_support', 'automatic_top_up', 'balance_not_updated_after_bank_transfer', 'balance_not_updated_after_cheque_or_cash_deposit', 'beneficiary_not_allowed', 'cancel_transfer', 'card_about_to_expire', 'card_acceptance', 'card_arrival', 'card_delivery_estimate', 'card_linking', 'card_not_working', 'card_payment_fee_charged', 'card_payment_not_recognised', 'card_payment_wrong_exchange_rate', 'card_swallowed', 'cash_withdrawal_charge', 'cash_withdrawal_not_recognised', 'change_pin', 'compromised_card', 'contactless_not_working', 'country_support', 'declined_card_payment', 'declined_cash_withdrawal', 'declined_transfer', 'direct_debit_payment_not_recognised', 'disposable_card_limits', 'edit_personal_details', 'exchange_charge', 'exchange_rate', 'exchange_via_app', 'extra_charge_on_statement', 'failed_transfer', 'fiat_currency_support', 'get_disposable_virtual_card', 'get_physical_card', 'getting_spare_card', 'getting_virtual_card', 'lost_or_stolen_card', 'lost_or_stolen_phone', 'order_physical_card', 'passcode_forgotten', 'pending_card_payment', 'pending_cash_withdrawal', 'pending_top_up', 'pending_transfer', 'pin_blocked', 'receiving_money', 'Refund_not_showing_up', 'request_refund', 'reverted_card_payment?', 'supported_cards_and_currencies', 'terminate_account', 'top_up_by_bank_transfer_charge', 'top_up_by_card_charge', 'top_up_by_cash_or_cheque', 'top_up_failed', 'top_up_limits', 'top_up_reverted', 'topping_up_by_card', 'transaction_charged_twice', 'transfer_fee_charged', 'transfer_into_account', 'transfer_not_received_by_recipient', 'transfer_timing', 'unable_to_verify_identity', 'verify_my_identity', 'verify_source_of_funds', 'verify_top_up', 'virtual_card_not_working', 'visa_or_mastercard', 'why_verify_identity', 'wrong_amount_of_cash_received', 'wrong_exchange_rate_for_cash_withdrawal'], id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am still waiting on my card? 11\n",
      "What can I do if my card still hasn't arrived after 2 weeks? 11\n",
      "I have been waiting over a week. Is the card still coming? 11\n",
      "Can I track my card while it is in the process of delivery? 11\n",
      "How do I know if I will get my card, or if it is lost? 11\n"
     ]
    }
   ],
   "source": [
    "for item,label in zip(dataset[\"train\"][\"text\"][:5],dataset[\"train\"][\"label\"][:5]):\n",
    "    print(item, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAFlCAYAAAApo6aBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVNElEQVR4nO3df6yeZ3kf8O9VsoLWlgDNaYgImUOUElOsGsmCaiWIJGwLJC2l2ggpYoGxGUvJuomgYbofnCFV87amaFNTogApAUEa2owWJZSCYjTCNlYcmmKCE5VQMxzlhyEQqlJoA9f+8GtzXuPY5rz38XvO8ecjHfm+7+d93udS/vrmup/3eaq7AwDA7H5k3gUAAKwXghUAwCCCFQDAIIIVAMAgghUAwCCCFQDAIKfMu4AkOe2003rDhg3zLgMA4JjuvPPOr3b3wpGOrYpgtWHDhuzatWveZQAAHFNVffnxjtkKBAAYRLACABhEsAIAGESwAgAYRLACABhEsAIAGESwAgAYRLACABhEsAIAGESwAgAYRLACABhEsAIAGESwAgAY5JR5FwA/rA3bbzs03rvjkjlWAgDTdKwAAAYRrAAABhGsAAAGEawAAAYRrAAABhGsAAAGEawAAAbxHCvWtsVTD5s/Op86ACA6VgAAw+hYsa5sunHT1Hz3FbvnVAkAJyMdKwCAQXSsWNf2nLfx0HjjPXvmWAkAJwMdKwCAQQQrAIBBBCsAgEEEKwCAQY5583pV3ZDk0iQPd/dzJ2s3J3n25CNPSfKN7t5cVRuS7Ely7+TYp7t72+iiYTmu3bZzan7ldRfOqRIA1qvj+VXge5L8VpL3Hlzo7ssOjqvqmiRLH3d9X3dvHlQfAMCaccxg1d2fnHSifkBVVZJXJvG//gDASW/W51idn+Sh7v7zJWtnV9WfJvlmkn/X3Xcc6cSq2ppka5KcddZZM5YBP7xrLrt0an71zbfOqRIA1otZb16/PMlNS+YPJDmru5+X5I1JPlBVTz7Sid19fXdv6e4tCwsLM5YBADB/yw5WVXVKkl9OcvPBte7+Tnd/bTK+M8l9SX561iIBANaCWbYCX5Lknu7ed3ChqhaSPNLd362qZyU5N8mXZqwRToh927+/a33mjvPnWAkAa9UxO1ZVdVOS/5Pk2VW1r6pePzn0qkxvAybJi5J8rqruSvL7SbZ19yMD6wUAWLWO51eBlz/O+muPsHZLkltmLwvma3Fx8ahzADgST14HABhk1sctwEnh9p3nHBq/uqabsg9esPkEVwPAaqVjBQAwiGAFADCIrUCY0Ybtt03N9+64ZE6VADBvOlYAAIPoWMFoi6cuGT86vzoAOOEEK1hBm27cNDXffcXuOVUCwIlgKxAAYBAdKziB9py3cWq+8Z49c6oEgJWgYwUAMIhgBQAwiGAFADCIYAUAMIib12GOrt2289D4yusunGMlAIygYwUAMIiOFawS11x26dT86ptvnVMlACyXYAWr1L7tdxwan7nj/DlWAsDxshUIADCIjhWsAYuLi0edA7A66FgBAAyiYwVr0O07z5mav7puOTR+8ILNJ7gaAA7SsQIAGESwAgAYxFYgrDMbtt82Nd+745I5VQJw8tGxAgAYRMcK1rvFUw+bPzqfOgBOAjpWAACDCFYAAIMIVgAAgwhWAACDuHkdTjKbbtx0aPzB//TY1LGN9+w50eUArCvH7FhV1Q1V9XBVfX7J2mJV3V9Vd03+Xrbk2Fuq6otVdW9V/aOVKhwAYLU5nq3A9yS5+Ajrb+/uzZO/jyRJVT0nyauS/MzknN+uqieMKhYAYDU7ZrDq7k8meeQ4v+/lSX63u7/T3X+R5ItJnj9DfQAAa8YsN69fVVWfm2wVPnWy9owkX1nymX2TNQCAdW+5weodSc5JsjnJA0mu+WG/oKq2VtWuqtq1f//+ZZYBALB6LOtXgd390MFxVb0zya2T6f1Jnrnko2dO1o70HdcnuT5JtmzZ0supAxjr2m07D42vvO7COVYCsDYtq2NVVWcsmb4iycFfDH44yauq6olVdXaSc5P8yWwlAgCsDcfsWFXVTUlenOS0qtqX5K1JXlxVm5N0kr1J3pAk3X13VX0wyReSPJbkyu7+7opUDqyoay67dGp+9c23Ps4nATjomMGquy8/wvK7j/L5X0/y67MUBQCwFnmlDQDAIF5pAxyXfdvvmJqfueP8OVUCsHrpWAEADKJjBSzL4uLiEccAJzMdKwCAQQQrAIBBbAUCM7t95zlT84suvG9OlQDMl44VAMAgghUAwCCCFQDAIIIVAMAgghUAwCCCFQDAIB63AAz39E/cNTV/8ILNc6kD4ETTsQIAGESwAgAYxFYgsOI2bL/t0HjvjkvmWAnAytKxAgAYRLACABhEsAIAGMQ9VsCJtXjqYfNH51MHwArQsQIAGESwAgAYxFYgMFebbtx0aLz7it1zrARgdjpWAACDCFYAAIMIVgAAgwhWAACDCFYAAIMIVgAAg3jcArBq7Dlv49R854uvnZp/++u/eWh82dlvnjp25o7zV64wgOMkWAHrwuLi4lHnACeCrUAAgEGO2bGqqhuSXJrk4e5+7mTtvyb5hSR/k+S+JK/r7m9U1YYke5LcOzn90929bSUKBzia23eec2h80YX3zbES4GRyPFuB70nyW0neu2Tt40ne0t2PVdV/TvKWJAdveLivuzePLBJgFk//xF1T8yf98f2Hxnuf9CvTH1589ARUBKxXx9wK7O5PJnnksLWPdfdjk+mnk5y5ArUBAKwpI+6x+mdJ/mjJ/Oyq+tOq+p9V9bg/06mqrVW1q6p27d+/f0AZAADzNVOwqqp/m+SxJO+fLD2Q5Kzufl6SNyb5QFU9+Ujndvf13b2lu7csLCzMUgYAwKqw7GBVVa/NgZvaX93dnSTd/Z3u/tpkfGcO3Nj+0wPqBABY9ZYVrKrq4iT/Jskvdve3lqwvVNUTJuNnJTk3yZdGFAoAsNodz+MWbkry4iSnVdW+JG/NgV8BPjHJx6sq+f5jFV6U5G1V9bdJvpdkW3c/csQvBgBYZ44ZrLr78iMsv/txPntLkltmLQpgXjbduGlqvvuK3XOqBFiLPHkdAGAQ7woEOIqlL4Y+2kuhk+kXQ7/rSbdPHfPuQjg5CFYAJ4BX7MDJwVYgAMAgghUAwCCCFQDAIO6xAjjBnv6Ju6bmD16weS51AOPpWAEADCJYAQAMIlgBAAwiWAEADOLmdYA527D9tqn53h2XzKkSYFaCFcBqs3jqoeGms8+aOuSl0LC62QoEABhExwpgDTnaS6GvvO7CE10OcBgdKwCAQXSsANaJay67dGp+2dlvPjR+15Nunzq2uLh4IkqCk45gBXASun3nOVPziy68b06VwPpiKxAAYBAdKwCmXgztpdCwfDpWAACDCFYAAIPYCgRgilfswPLpWAEADCJYAQAMIlgBAAziHisAjm7x1CXjR+dXB6wBghUAx23TjZum5ruv2D2nSmB1shUIADCIjhUAy7bnvI1T84337JlTJbA66FgBAAyiYwXAMNdu23lofOV1F86xEpiP4wpWVXVDkkuTPNzdz52sPS3JzUk2JNmb5JXd/fWqqiT/LcnLknwryWu7+7PjSwdgNbvmskun5lfffOucKoET53i3At+T5OLD1rYnub27z01y+2SeJC9Ncu7kb2uSd8xeJgDA6ndcwaq7P5nkkcOWX57kxsn4xiS/tGT9vX3Ap5M8parOGFArAMCqNsvN66d39wOT8YNJTp+Mn5HkK0s+t2+yBgCwrg35VWB3d5L+Yc6pqq1Vtauqdu3fv39EGQAAczVLsHro4Bbf5N+HJ+v3J3nmks+dOVmb0t3Xd/eW7t6ysLAwQxkAAKvDLMHqw0mumIyvSPKHS9b/aR3wc0keXbJlCACwbh3v4xZuSvLiJKdV1b4kb02yI8kHq+r1Sb6c5JWTj38kBx618MUceNzC6wbXDMAatG/7HVPzM3ecP6dKYOUcV7Dq7ssf59BFR/hsJ7lylqIAANYiT14HYC4WFxcPjc9/0fumjl104X0nuBoYw7sCAQAGEawAAAaxFQjAqvP0T9x1aPzgBZvnVgf8sHSsAAAGEawAAAYRrAAABhGsAAAGEawAAAYRrAAABhGsAAAGEawAAAYRrAAABhGsAAAGEawAAAYRrAAABvESZgBWtQ3bb5ua791xyZwqgWPTsQIAGETHCoC1ZfHUw+aPzqcOOAIdKwCAQQQrAIBBBCsAgEEEKwCAQQQrAIBBBCsAgEE8bgGANW3TjZsOjXdfsXuOlYCOFQDAMIIVAMAgghUAwCCCFQDAIIIVAMAgghUAwCCCFQDAIMt+jlVVPTvJzUuWnpXkPyR5SpJ/kWT/ZP3Xuvsjy70OAMBasexg1d33JtmcJFX1hCT3J/lQktcleXt3/8aIAgEA1opRW4EXJbmvu7886PsAANacUcHqVUluWjK/qqo+V1U3VNVTj3RCVW2tql1VtWv//v1H+ggAwJoyc7Cqqh9N8otJfm+y9I4k5+TANuEDSa450nndfX13b+nuLQsLC7OWAQAwdyM6Vi9N8tnufihJuvuh7v5ud38vyTuTPH/ANQAAVr0RweryLNkGrKozlhx7RZLPD7gGAMCqt+xfBSZJVf1Ykn+Q5A1Llv9LVW1O0kn2HnYMAGDdmilYdfdfJfnJw9ZeM1NFAABrlCevAwAMIlgBAAwiWAEADCJYAQAMIlgBAAwiWAEADCJYAQAMIlgBAAwiWAEADCJYAQAMIlgBAAwiWAEADCJYAQAMIlgBAAwiWAEADCJYAQAMIlgBAAxyyrwLAIBR9py3cWq+8Z49c6qEk5WOFQDAIIIVAMAgghUAwCDusQJg3bp2286p+ZXXXTinSjhZ6FgBAAwiWAEADCJYAQAMIlgBAAwiWAEADCJYAQAMIlgBAAwiWAEADCJYAQAMIlgBAAwy8yttqmpvkr9M8t0kj3X3lqp6WpKbk2xIsjfJK7v767NeCwBgNRvVsbqguzd395bJfHuS27v73CS3T+YAAOvaSm0FvjzJjZPxjUl+aYWuAwCwaowIVp3kY1V1Z1Vtnayd3t0PTMYPJjl9wHUAAFa1me+xSvLC7r6/qn4qycer6p6lB7u7q6oPP2kSwrYmyVlnnTWgDACA+Zq5Y9Xd90/+fTjJh5I8P8lDVXVGkkz+ffgI513f3Vu6e8vCwsKsZQAAzN1MwaqqfqyqfuLgOMk/TPL5JB9OcsXkY1ck+cNZrgMAsBbMuhV4epIPVdXB7/pAd3+0qj6T5INV9fokX07yyhmvAwCw6s0UrLr7S0l+9gjrX0ty0SzfDQCw1njyOgDAIIIVAMAgghUAwCCCFQDAIIIVAMAgghUAwCCCFQDAIIIVAMAgghUAwCCCFQDAILO+KxAA1oxrLrv00Pjqm2+dYyWsVzpWAACDCFYAAIMIVgAAgwhWAACDCFYAAIMIVgAAgwhWAACDCFYAAIMIVgAAg3jyOgAnpX3b75ian7nj/DlVwnqiYwUAMIhgBQAwiGAFADCIYAUAMIhgBQAwiGAFADCIYAUAMIhgBQAwiGAFADCIYAUAMIhgBQAwiGAFADDIsoNVVT2zqj5RVV+oqrur6l9N1her6v6qumvy97Jx5QIArF6nzHDuY0mu7u7PVtVPJLmzqj4+Ofb27v6N2csDAFg7lh2suvuBJA9Mxn9ZVXuSPGNUYQAAa82Qe6yqakOS5yX5v5Olq6rqc1V1Q1U99XHO2VpVu6pq1/79+0eUAQAwVzMHq6r68SS3JPnX3f3NJO9Ick6SzTnQ0brmSOd19/XdvaW7tywsLMxaBgDA3M0UrKrq7+RAqHp/d/+PJOnuh7r7u939vSTvTPL82csEAFj9ZvlVYCV5d5I93f2bS9bPWPKxVyT5/PLLAwBYO2b5VeDPJ3lNkt1Vdddk7deSXF5Vm5N0kr1J3jDDNQAA1oxZfhX4qSR1hEMfWX45AABrlyevAwAMIlgBAAwiWAEADCJYAQAMIlgBAAwiWAEADCJYAQAMIlgBAAwiWAEADCJYAQAMIlgBAAwiWAEADCJYAQAMIlgBAAwiWAEADCJYAQAMIlgBAAwiWAEADCJYAQAMIlgBAAwiWAEADCJYAQAMIlgBAAwiWAEADCJYAQAMIlgBAAwiWAEADCJYAQAMIlgBAAwiWAEADCJYAQAMIlgBAAyyYsGqqi6uqnur6otVtX2lrgMAsFqsSLCqqickuTbJS5M8J8nlVfWclbgWAMBqsVIdq+cn+WJ3f6m7/ybJ7yZ5+QpdCwBgVVipYPWMJF9ZMt83WQMAWLequ8d/adU/TnJxd//zyfw1SV7Q3Vct+czWJFsn02cnuXd4IcDJ7rQkX513EcC68/e6e+FIB05ZoQven+SZS+ZnTtYO6e7rk1y/QtcHSFXt6u4t864DOHms1FbgZ5KcW1VnV9WPJnlVkg+v0LUAAFaFFelYdfdjVXVVkj9O8oQkN3T33StxLQCA1WJF7rECWA2qauvktgOAE0KwAgAYxCttAAAGEayAuamqX62qPVX1/nnXAjCCrUBgbqrqniQv6e59S9ZO6e7H5lgWwLLpWAFzUVXXJXlWkj+qqker6n1V9b+SvK+qFqrqlqr6zOTv5yfn/GRVfayq7q6qd1XVl6vqtKraUFWfX/Ldb6qqxcn4nKr6aFXdWVV3VNV5k/X3VNV/r6r/XVVfmjzY+OD5b66q3VX1Z1W1Y/Idn11y/Nylc4CDVuoBoQBH1d3bquriJBckuSrJLyR5YXf/dVV9IMnbu/tTVXVWDjy6ZWOStyb5VHe/raouSfL647jU9Um2dfefV9ULkvx2kgsnx85I8sIk5+XAs/Z+v6pemgPvNn1Bd3+rqp7W3Y9Mwt/m7r4ryeuS/M6Y/xLAeiJYAavFh7v7ryfjlyR5TlUdPPbkqvrxJC9K8stJ0t23VdXXj/aFk3P+fpLfW/JdT1zykT/o7u8l+UJVnb7k2r/T3d+aXOeRyfq7kryuqt6Y5LIceNk8wBTBClgt/mrJ+EeS/Fx3f3vpB5aEo8M9lulbG5605Hu+0d2bH+e87yz9+mPUd0sOdMx2Jrmzu792jM8DJyH3WAGr0ceS/MuDk6raPBl+MsmvTNZemuSpk/WHkvzU5B6sJya5NEm6+5tJ/qKq/snknKqqnz3GtT+eA52pvzs552mT7/p2DmxJviO2AYHHIVgBq9GvJtlSVZ+rqi8k2TZZ/49JXlRVd+fAluD/S5Lu/tskb0vyJzkQjO5Z8l2vTvL6qvqzJHfnwP1Tj6u7P5oD91vtqqq7krxpyeH3J/leDgQ/gB/gcQvAmlVVe5Ns6e6vnqDrvSnJqd3970/E9YC1xz1WAMehqj6U5Jx8/xeFAD9AxwoAYBD3WAEADCJYAQAMIlgBAAwiWAEADCJYAQAMIlgBAAzy/wHzElsxRz6f2QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "train_texts = [item[\"text\"] for item in dataset[\"train\"]]\n",
    "train_labels = [item[\"label\"] for item in dataset[\"train\"]]\n",
    "\n",
    "test_texts = [item[\"text\"] for item in dataset[\"test\"]]\n",
    "test_labels = [item[\"label\"] for item in dataset[\"test\"]]\n",
    "\n",
    "label_counter = Counter(train_labels)\n",
    "label_names = dataset[\"train\"].features[\"label\"].names\n",
    "label_frequencies = {label_names[label]: [label_counter[label]] for label in label_counter}\n",
    "\n",
    "df = pd.DataFrame.from_dict(label_frequencies, orient=\"index\", columns=[\"frequency\"])\n",
    "df = df.sort_values(\"frequency\", ascending=False)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "ax = df.transpose().plot(kind=\"bar\", rot=0)\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 9002\n",
      "Dev: 1001\n",
      "Test: 3080\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(train_texts, \n",
    "                                                                    train_labels, \n",
    "                                                                    test_size=0.1, \n",
    "                                                                    shuffle=True, \n",
    "                                                                    random_state=1)\n",
    "\n",
    "print(\"Train:\", len(train_texts))\n",
    "print(\"Dev:\", len(dev_texts))\n",
    "print(\"Test:\", len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48,\n",
       " 46,\n",
       " 53,\n",
       " 74,\n",
       " 35,\n",
       " 30,\n",
       " 68,\n",
       " 66,\n",
       " 16,\n",
       " 31,\n",
       " 9,\n",
       " 70,\n",
       " 13,\n",
       " 9,\n",
       " 2,\n",
       " 26,\n",
       " 19,\n",
       " 21,\n",
       " 16,\n",
       " 12,\n",
       " 16,\n",
       " 0,\n",
       " 2,\n",
       " 67,\n",
       " 76,\n",
       " 31,\n",
       " 7,\n",
       " 76,\n",
       " 45,\n",
       " 12,\n",
       " 17,\n",
       " 71,\n",
       " 28,\n",
       " 20,\n",
       " 59,\n",
       " 48,\n",
       " 34,\n",
       " 35,\n",
       " 71,\n",
       " 52,\n",
       " 47,\n",
       " 7,\n",
       " 35,\n",
       " 21,\n",
       " 73,\n",
       " 38,\n",
       " 45,\n",
       " 70,\n",
       " 37,\n",
       " 73,\n",
       " 62,\n",
       " 2,\n",
       " 52,\n",
       " 21,\n",
       " 34,\n",
       " 67,\n",
       " 56,\n",
       " 39,\n",
       " 53,\n",
       " 51,\n",
       " 62,\n",
       " 5,\n",
       " 2,\n",
       " 63,\n",
       " 43,\n",
       " 28,\n",
       " 27,\n",
       " 63,\n",
       " 17,\n",
       " 75,\n",
       " 56,\n",
       " 52,\n",
       " 6,\n",
       " 48,\n",
       " 36,\n",
       " 11,\n",
       " 43,\n",
       " 5,\n",
       " 74,\n",
       " 7,\n",
       " 3,\n",
       " 53,\n",
       " 44,\n",
       " 12,\n",
       " 19,\n",
       " 15,\n",
       " 42,\n",
       " 7,\n",
       " 59,\n",
       " 73,\n",
       " 18,\n",
       " 10,\n",
       " 33,\n",
       " 61,\n",
       " 60,\n",
       " 73,\n",
       " 41,\n",
       " 4,\n",
       " 53,\n",
       " 15,\n",
       " 37,\n",
       " 69,\n",
       " 65,\n",
       " 47,\n",
       " 22,\n",
       " 51,\n",
       " 11,\n",
       " 46,\n",
       " 37,\n",
       " 55,\n",
       " 68,\n",
       " 2,\n",
       " 73,\n",
       " 5,\n",
       " 49,\n",
       " 21,\n",
       " 42,\n",
       " 54,\n",
       " 46,\n",
       " 14,\n",
       " 33,\n",
       " 67,\n",
       " 35,\n",
       " 8,\n",
       " 40,\n",
       " 36,\n",
       " 27,\n",
       " 50,\n",
       " 63,\n",
       " 11,\n",
       " 15,\n",
       " 48,\n",
       " 43,\n",
       " 48,\n",
       " 69,\n",
       " 13,\n",
       " 32,\n",
       " 70,\n",
       " 76,\n",
       " 21,\n",
       " 6,\n",
       " 62,\n",
       " 62,\n",
       " 22,\n",
       " 57,\n",
       " 64,\n",
       " 61,\n",
       " 51,\n",
       " 26,\n",
       " 20,\n",
       " 65,\n",
       " 15,\n",
       " 76,\n",
       " 36,\n",
       " 0,\n",
       " 73,\n",
       " 61,\n",
       " 48,\n",
       " 19,\n",
       " 59,\n",
       " 6,\n",
       " 75,\n",
       " 63,\n",
       " 37,\n",
       " 62,\n",
       " 61,\n",
       " 20,\n",
       " 53,\n",
       " 31,\n",
       " 60,\n",
       " 2,\n",
       " 38,\n",
       " 59,\n",
       " 24,\n",
       " 72,\n",
       " 39,\n",
       " 51,\n",
       " 6,\n",
       " 39,\n",
       " 56,\n",
       " 67,\n",
       " 32,\n",
       " 34,\n",
       " 27,\n",
       " 10,\n",
       " 75,\n",
       " 29,\n",
       " 69,\n",
       " 16,\n",
       " 35,\n",
       " 13,\n",
       " 45,\n",
       " 8,\n",
       " 25,\n",
       " 4,\n",
       " 59,\n",
       " 26,\n",
       " 61,\n",
       " 26,\n",
       " 17,\n",
       " 63,\n",
       " 23,\n",
       " 43,\n",
       " 38,\n",
       " 34,\n",
       " 34,\n",
       " 57,\n",
       " 66,\n",
       " 52,\n",
       " 16,\n",
       " 69,\n",
       " 32,\n",
       " 1,\n",
       " 51,\n",
       " 32,\n",
       " 70,\n",
       " 35,\n",
       " 50,\n",
       " 24,\n",
       " 16,\n",
       " 48,\n",
       " 34,\n",
       " 76,\n",
       " 66,\n",
       " 6,\n",
       " 53,\n",
       " 13,\n",
       " 66,\n",
       " 69,\n",
       " 14,\n",
       " 40,\n",
       " 39,\n",
       " 21,\n",
       " 70,\n",
       " 11,\n",
       " 66,\n",
       " 68,\n",
       " 32,\n",
       " 53,\n",
       " 8,\n",
       " 13,\n",
       " 48,\n",
       " 8,\n",
       " 46,\n",
       " 58,\n",
       " 26,\n",
       " 52,\n",
       " 65,\n",
       " 26,\n",
       " 23,\n",
       " 28,\n",
       " 43,\n",
       " 48,\n",
       " 35,\n",
       " 15,\n",
       " 27,\n",
       " 12,\n",
       " 53,\n",
       " 5,\n",
       " 41,\n",
       " 63,\n",
       " 32,\n",
       " 0,\n",
       " 45,\n",
       " 27,\n",
       " 15,\n",
       " 54,\n",
       " 59,\n",
       " 48,\n",
       " 3,\n",
       " 9,\n",
       " 30,\n",
       " 68,\n",
       " 0,\n",
       " 33,\n",
       " 21,\n",
       " 4,\n",
       " 62,\n",
       " 15,\n",
       " 54,\n",
       " 32,\n",
       " 39,\n",
       " 24,\n",
       " 35,\n",
       " 44,\n",
       " 51,\n",
       " 74,\n",
       " 55,\n",
       " 63,\n",
       " 4,\n",
       " 9,\n",
       " 71,\n",
       " 13,\n",
       " 51,\n",
       " 57,\n",
       " 64,\n",
       " 33,\n",
       " 5,\n",
       " 35,\n",
       " 20,\n",
       " 18,\n",
       " 29,\n",
       " 8,\n",
       " 11,\n",
       " 57,\n",
       " 41,\n",
       " 69,\n",
       " 9,\n",
       " 64,\n",
       " 44,\n",
       " 70,\n",
       " 8,\n",
       " 34,\n",
       " 62,\n",
       " 70,\n",
       " 64,\n",
       " 74,\n",
       " 35,\n",
       " 45,\n",
       " 74,\n",
       " 28,\n",
       " 8,\n",
       " 74,\n",
       " 67,\n",
       " 4,\n",
       " 58,\n",
       " 43,\n",
       " 27,\n",
       " 26,\n",
       " 0,\n",
       " 15,\n",
       " 28,\n",
       " 66,\n",
       " 24,\n",
       " 70,\n",
       " 59,\n",
       " 7,\n",
       " 44,\n",
       " 56,\n",
       " 28,\n",
       " 15,\n",
       " 66,\n",
       " 41,\n",
       " 16,\n",
       " 46,\n",
       " 26,\n",
       " 29,\n",
       " 25,\n",
       " 6,\n",
       " 6,\n",
       " 25,\n",
       " 69,\n",
       " 45,\n",
       " 40,\n",
       " 61,\n",
       " 64,\n",
       " 55,\n",
       " 57,\n",
       " 37,\n",
       " 31,\n",
       " 47,\n",
       " 54,\n",
       " 67,\n",
       " 65,\n",
       " 68,\n",
       " 51,\n",
       " 8,\n",
       " 44,\n",
       " 75,\n",
       " 39,\n",
       " 60,\n",
       " 34,\n",
       " 76,\n",
       " 63,\n",
       " 5,\n",
       " 36,\n",
       " 47,\n",
       " 36,\n",
       " 33,\n",
       " 75,\n",
       " 21,\n",
       " 51,\n",
       " 4,\n",
       " 38,\n",
       " 52,\n",
       " 74,\n",
       " 59,\n",
       " 35,\n",
       " 42,\n",
       " 24,\n",
       " 71,\n",
       " 17,\n",
       " 16,\n",
       " 51,\n",
       " 74,\n",
       " 70,\n",
       " 42,\n",
       " 68,\n",
       " 28,\n",
       " 25,\n",
       " 47,\n",
       " 3,\n",
       " 45,\n",
       " 15,\n",
       " 42,\n",
       " 76,\n",
       " 50,\n",
       " 74,\n",
       " 45,\n",
       " 20,\n",
       " 69,\n",
       " 4,\n",
       " 0,\n",
       " 25,\n",
       " 26,\n",
       " 16,\n",
       " 4,\n",
       " 53,\n",
       " 42,\n",
       " 35,\n",
       " 17,\n",
       " 19,\n",
       " 31,\n",
       " 49,\n",
       " 40,\n",
       " 71,\n",
       " 25,\n",
       " 5,\n",
       " 11,\n",
       " 63,\n",
       " 62,\n",
       " 71,\n",
       " 75,\n",
       " 0,\n",
       " 52,\n",
       " 11,\n",
       " 67,\n",
       " 34,\n",
       " 20,\n",
       " 25,\n",
       " 13,\n",
       " 28,\n",
       " 45,\n",
       " 38,\n",
       " 28,\n",
       " 6,\n",
       " 55,\n",
       " 65,\n",
       " 19,\n",
       " 59,\n",
       " 58,\n",
       " 25,\n",
       " 15,\n",
       " 15,\n",
       " 35,\n",
       " 32,\n",
       " 64,\n",
       " 76,\n",
       " 73,\n",
       " 47,\n",
       " 73,\n",
       " 15,\n",
       " 4,\n",
       " 54,\n",
       " 58,\n",
       " 30,\n",
       " 35,\n",
       " 25,\n",
       " 61,\n",
       " 31,\n",
       " 70,\n",
       " 42,\n",
       " 22,\n",
       " 9,\n",
       " 12,\n",
       " 72,\n",
       " 72,\n",
       " 22,\n",
       " 27,\n",
       " 73,\n",
       " 20,\n",
       " 0,\n",
       " 74,\n",
       " 14,\n",
       " 45,\n",
       " 52,\n",
       " 69,\n",
       " 13,\n",
       " 10,\n",
       " 45,\n",
       " 19,\n",
       " 27,\n",
       " 13,\n",
       " 52,\n",
       " 33,\n",
       " 74,\n",
       " 11,\n",
       " 61,\n",
       " 66,\n",
       " 39,\n",
       " 12,\n",
       " 58,\n",
       " 17,\n",
       " 11,\n",
       " 33,\n",
       " 61,\n",
       " 64,\n",
       " 34,\n",
       " 67,\n",
       " 64,\n",
       " 41,\n",
       " 26,\n",
       " 65,\n",
       " 2,\n",
       " 6,\n",
       " 75,\n",
       " 55,\n",
       " 65,\n",
       " 47,\n",
       " 4,\n",
       " 22,\n",
       " 3,\n",
       " 57,\n",
       " 32,\n",
       " 29,\n",
       " 43,\n",
       " 44,\n",
       " 5,\n",
       " 52,\n",
       " 48,\n",
       " 71,\n",
       " 14,\n",
       " 64,\n",
       " 33,\n",
       " 56,\n",
       " 13,\n",
       " 57,\n",
       " 47,\n",
       " 65,\n",
       " 58,\n",
       " 48,\n",
       " 46,\n",
       " 63,\n",
       " 18,\n",
       " 1,\n",
       " 55,\n",
       " 7,\n",
       " 32,\n",
       " 62,\n",
       " 2,\n",
       " 6,\n",
       " 11,\n",
       " 26,\n",
       " 19,\n",
       " 7,\n",
       " 13,\n",
       " 50,\n",
       " 75,\n",
       " 54,\n",
       " 63,\n",
       " 76,\n",
       " 31,\n",
       " 47,\n",
       " 5,\n",
       " 28,\n",
       " 68,\n",
       " 70,\n",
       " 60,\n",
       " 49,\n",
       " 75,\n",
       " 32,\n",
       " 27,\n",
       " 76,\n",
       " 19,\n",
       " 1,\n",
       " 5,\n",
       " 43,\n",
       " 46,\n",
       " 59,\n",
       " 18,\n",
       " 73,\n",
       " 47,\n",
       " 12,\n",
       " 34,\n",
       " 68,\n",
       " 50,\n",
       " 63,\n",
       " 13,\n",
       " 17,\n",
       " 3,\n",
       " 71,\n",
       " 15,\n",
       " 75,\n",
       " 15,\n",
       " 68,\n",
       " 43,\n",
       " 69,\n",
       " 42,\n",
       " 25,\n",
       " 39,\n",
       " 70,\n",
       " 75,\n",
       " 16,\n",
       " 8,\n",
       " 2,\n",
       " 30,\n",
       " 62,\n",
       " 17,\n",
       " 3,\n",
       " 47,\n",
       " 28,\n",
       " 73,\n",
       " 28,\n",
       " 75,\n",
       " 48,\n",
       " 32,\n",
       " 24,\n",
       " 16,\n",
       " 58,\n",
       " 60,\n",
       " 73,\n",
       " 30,\n",
       " 32,\n",
       " 71,\n",
       " 71,\n",
       " 68,\n",
       " 73,\n",
       " 20,\n",
       " 19,\n",
       " 38,\n",
       " 0,\n",
       " 52,\n",
       " 31,\n",
       " 24,\n",
       " 31,\n",
       " 13,\n",
       " 22,\n",
       " 20,\n",
       " 26,\n",
       " 55,\n",
       " 73,\n",
       " 41,\n",
       " 52,\n",
       " 54,\n",
       " 66,\n",
       " 69,\n",
       " 50,\n",
       " 26,\n",
       " 2,\n",
       " 26,\n",
       " 44,\n",
       " 29,\n",
       " 75,\n",
       " 52,\n",
       " 31,\n",
       " 73,\n",
       " 15,\n",
       " 45,\n",
       " 52,\n",
       " 6,\n",
       " 13,\n",
       " 45,\n",
       " 54,\n",
       " 66,\n",
       " 1,\n",
       " 35,\n",
       " 20,\n",
       " 16,\n",
       " 5,\n",
       " 13,\n",
       " 26,\n",
       " 41,\n",
       " 54,\n",
       " 63,\n",
       " 11,\n",
       " 15,\n",
       " 17,\n",
       " 46,\n",
       " 45,\n",
       " 5,\n",
       " 8,\n",
       " 47,\n",
       " 61,\n",
       " 67,\n",
       " 12,\n",
       " 63,\n",
       " 16,\n",
       " 46,\n",
       " 27,\n",
       " 7,\n",
       " 6,\n",
       " 62,\n",
       " 75,\n",
       " 67,\n",
       " 2,\n",
       " 65,\n",
       " 34,\n",
       " 68,\n",
       " 5,\n",
       " 63,\n",
       " 51,\n",
       " 32,\n",
       " 66,\n",
       " 64,\n",
       " 29,\n",
       " 4,\n",
       " 49,\n",
       " 22,\n",
       " 22,\n",
       " 43,\n",
       " 49,\n",
       " 35,\n",
       " 33,\n",
       " 59,\n",
       " 55,\n",
       " 51,\n",
       " 75,\n",
       " 42,\n",
       " 4,\n",
       " 48,\n",
       " 25,\n",
       " 15,\n",
       " 48,\n",
       " 11,\n",
       " 3,\n",
       " 21,\n",
       " 10,\n",
       " 11,\n",
       " 28,\n",
       " 70,\n",
       " 48,\n",
       " 53,\n",
       " 70,\n",
       " 22,\n",
       " 53,\n",
       " 74,\n",
       " 58,\n",
       " 24,\n",
       " 63,\n",
       " 74,\n",
       " 42,\n",
       " 38,\n",
       " 16,\n",
       " 23,\n",
       " 5,\n",
       " 47,\n",
       " 5,\n",
       " 45,\n",
       " 47,\n",
       " 15,\n",
       " 5,\n",
       " 27,\n",
       " 41,\n",
       " 36,\n",
       " 33,\n",
       " 51,\n",
       " 9,\n",
       " 53,\n",
       " 14,\n",
       " 40,\n",
       " 15,\n",
       " 47,\n",
       " 35,\n",
       " 71,\n",
       " 57,\n",
       " 20,\n",
       " 47,\n",
       " 74,\n",
       " 4,\n",
       " 40,\n",
       " 20,\n",
       " 14,\n",
       " 74,\n",
       " 64,\n",
       " 48,\n",
       " 30,\n",
       " 72,\n",
       " 35,\n",
       " 68,\n",
       " 46,\n",
       " 26,\n",
       " 5,\n",
       " 76,\n",
       " 27,\n",
       " 63,\n",
       " 34,\n",
       " 17,\n",
       " 76,\n",
       " 43,\n",
       " 26,\n",
       " 15,\n",
       " 4,\n",
       " 56,\n",
       " 75,\n",
       " 4,\n",
       " 39,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 62,\n",
       " 8,\n",
       " 15,\n",
       " 7,\n",
       " 25,\n",
       " 52,\n",
       " 45,\n",
       " 43,\n",
       " 68,\n",
       " 28,\n",
       " 43,\n",
       " 49,\n",
       " 29,\n",
       " 76,\n",
       " 26,\n",
       " 72,\n",
       " 71,\n",
       " 38,\n",
       " 26,\n",
       " 39,\n",
       " 60,\n",
       " 28,\n",
       " 17,\n",
       " 7,\n",
       " 70,\n",
       " 21,\n",
       " 17,\n",
       " 53,\n",
       " 62,\n",
       " 19,\n",
       " 51,\n",
       " 39,\n",
       " 45,\n",
       " 67,\n",
       " 7,\n",
       " 32,\n",
       " 60,\n",
       " 8,\n",
       " 21,\n",
       " 33,\n",
       " 65,\n",
       " 64,\n",
       " 56,\n",
       " 47,\n",
       " 4,\n",
       " 3,\n",
       " 64,\n",
       " 47,\n",
       " 66,\n",
       " 49,\n",
       " 22,\n",
       " 14,\n",
       " 20,\n",
       " 41,\n",
       " 6,\n",
       " 27,\n",
       " 33,\n",
       " 50,\n",
       " 48,\n",
       " 13,\n",
       " 49,\n",
       " 17,\n",
       " 49,\n",
       " 68,\n",
       " 4,\n",
       " 2,\n",
       " 42,\n",
       " 76,\n",
       " 51,\n",
       " 32,\n",
       " 32,\n",
       " 66,\n",
       " 70,\n",
       " 68,\n",
       " 66,\n",
       " 30,\n",
       " 28,\n",
       " 3,\n",
       " 75,\n",
       " 19,\n",
       " 34,\n",
       " 17,\n",
       " 61,\n",
       " 76,\n",
       " 2,\n",
       " 34,\n",
       " 69,\n",
       " 40,\n",
       " 48,\n",
       " 64,\n",
       " 68,\n",
       " 54,\n",
       " 28,\n",
       " 76,\n",
       " 28,\n",
       " 69,\n",
       " 8,\n",
       " 3,\n",
       " 17,\n",
       " 62,\n",
       " 8,\n",
       " 29,\n",
       " 75,\n",
       " 28,\n",
       " 48,\n",
       " 4,\n",
       " 16,\n",
       " 14,\n",
       " 67,\n",
       " 75,\n",
       " 53,\n",
       " 68,\n",
       " 58,\n",
       " 0,\n",
       " 51,\n",
       " 9,\n",
       " 15,\n",
       " 66,\n",
       " 76,\n",
       " 41,\n",
       " 28,\n",
       " 4,\n",
       " 57,\n",
       " 20,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 13,\n",
       " 52,\n",
       " 51,\n",
       " 5,\n",
       " 7,\n",
       " 47,\n",
       " 59,\n",
       " 20,\n",
       " 22,\n",
       " 6,\n",
       " 16,\n",
       " 76,\n",
       " 0,\n",
       " 5,\n",
       " 8,\n",
       " 30,\n",
       " 57,\n",
       " 53,\n",
       " 12,\n",
       " 47,\n",
       " 59,\n",
       " 17,\n",
       " 28,\n",
       " 42,\n",
       " 63,\n",
       " 47,\n",
       " 15,\n",
       " 45,\n",
       " 70,\n",
       " 20,\n",
       " 24,\n",
       " 8,\n",
       " 36,\n",
       " 62,\n",
       " 51,\n",
       " 51,\n",
       " 74,\n",
       " 53,\n",
       " 73,\n",
       " 27,\n",
       " 38,\n",
       " 32,\n",
       " 65,\n",
       " 60,\n",
       " 59,\n",
       " 48,\n",
       " 20,\n",
       " 49,\n",
       " 60,\n",
       " 7,\n",
       " 70,\n",
       " 18,\n",
       " 37,\n",
       " 22,\n",
       " 31,\n",
       " 20,\n",
       " 45,\n",
       " 64,\n",
       " 21,\n",
       " 11,\n",
       " 71,\n",
       " 12,\n",
       " 53,\n",
       " 37,\n",
       " 43,\n",
       " 75,\n",
       " 30,\n",
       " 11,\n",
       " 8,\n",
       " 19,\n",
       " 55,\n",
       " 43,\n",
       " 13,\n",
       " 42]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['label'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** xlm-roberta-large ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9002\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 13503\n",
      "  Number of trainable parameters = 559969357\n",
      "Trainer is attempting to log a value of \"{0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2', 3: 'LABEL_3', 4: 'LABEL_4', 5: 'LABEL_5', 6: 'LABEL_6', 7: 'LABEL_7', 8: 'LABEL_8', 9: 'LABEL_9', 10: 'LABEL_10', 11: 'LABEL_11', 12: 'LABEL_12', 13: 'LABEL_13', 14: 'LABEL_14', 15: 'LABEL_15', 16: 'LABEL_16', 17: 'LABEL_17', 18: 'LABEL_18', 19: 'LABEL_19', 20: 'LABEL_20', 21: 'LABEL_21', 22: 'LABEL_22', 23: 'LABEL_23', 24: 'LABEL_24', 25: 'LABEL_25', 26: 'LABEL_26', 27: 'LABEL_27', 28: 'LABEL_28', 29: 'LABEL_29', 30: 'LABEL_30', 31: 'LABEL_31', 32: 'LABEL_32', 33: 'LABEL_33', 34: 'LABEL_34', 35: 'LABEL_35', 36: 'LABEL_36', 37: 'LABEL_37', 38: 'LABEL_38', 39: 'LABEL_39', 40: 'LABEL_40', 41: 'LABEL_41', 42: 'LABEL_42', 43: 'LABEL_43', 44: 'LABEL_44', 45: 'LABEL_45', 46: 'LABEL_46', 47: 'LABEL_47', 48: 'LABEL_48', 49: 'LABEL_49', 50: 'LABEL_50', 51: 'LABEL_51', 52: 'LABEL_52', 53: 'LABEL_53', 54: 'LABEL_54', 55: 'LABEL_55', 56: 'LABEL_56', 57: 'LABEL_57', 58: 'LABEL_58', 59: 'LABEL_59', 60: 'LABEL_60', 61: 'LABEL_61', 62: 'LABEL_62', 63: 'LABEL_63', 64: 'LABEL_64', 65: 'LABEL_65', 66: 'LABEL_66', 67: 'LABEL_67', 68: 'LABEL_68', 69: 'LABEL_69', 70: 'LABEL_70', 71: 'LABEL_71', 72: 'LABEL_72', 73: 'LABEL_73', 74: 'LABEL_74', 75: 'LABEL_75', 76: 'LABEL_76'}\" for key \"id2label\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute. You can use `MLFLOW_FLATTEN_PARAMS` environment variable to flatten the parameters and avoid this message.\n",
      "Trainer is attempting to log a value of \"{'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5, 'LABEL_6': 6, 'LABEL_7': 7, 'LABEL_8': 8, 'LABEL_9': 9, 'LABEL_10': 10, 'LABEL_11': 11, 'LABEL_12': 12, 'LABEL_13': 13, 'LABEL_14': 14, 'LABEL_15': 15, 'LABEL_16': 16, 'LABEL_17': 17, 'LABEL_18': 18, 'LABEL_19': 19, 'LABEL_20': 20, 'LABEL_21': 21, 'LABEL_22': 22, 'LABEL_23': 23, 'LABEL_24': 24, 'LABEL_25': 25, 'LABEL_26': 26, 'LABEL_27': 27, 'LABEL_28': 28, 'LABEL_29': 29, 'LABEL_30': 30, 'LABEL_31': 31, 'LABEL_32': 32, 'LABEL_33': 33, 'LABEL_34': 34, 'LABEL_35': 35, 'LABEL_36': 36, 'LABEL_37': 37, 'LABEL_38': 38, 'LABEL_39': 39, 'LABEL_40': 40, 'LABEL_41': 41, 'LABEL_42': 42, 'LABEL_43': 43, 'LABEL_44': 44, 'LABEL_45': 45, 'LABEL_46': 46, 'LABEL_47': 47, 'LABEL_48': 48, 'LABEL_49': 49, 'LABEL_50': 50, 'LABEL_51': 51, 'LABEL_52': 52, 'LABEL_53': 53, 'LABEL_54': 54, 'LABEL_55': 55, 'LABEL_56': 56, 'LABEL_57': 57, 'LABEL_58': 58, 'LABEL_59': 59, 'LABEL_60': 60, 'LABEL_61': 61, 'LABEL_62': 62, 'LABEL_63': 63, 'LABEL_64': 64, 'LABEL_65': 65, 'LABEL_66': 66, 'LABEL_67': 67, 'LABEL_68': 68, 'LABEL_69': 69, 'LABEL_70': 70, 'LABEL_71': 71, 'LABEL_72': 72, 'LABEL_73': 73, 'LABEL_74': 74, 'LABEL_75': 75, 'LABEL_76': 76}\" for key \"label2id\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute. You can use `MLFLOW_FLATTEN_PARAMS` environment variable to flatten the parameters and avoid this message.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcaa8cec908841e78d29162441749ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_42720\\48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 8.00 GiB total capacity; 7.20 GiB already allocated; 0 bytes free; 7.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nicol\\Desktop\\Personal\\conversational_ai\\test_stuff\\xlm_roBERTa.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/Desktop/Personal/conversational_ai/test_stuff/xlm_roBERTa.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/Desktop/Personal/conversational_ai/test_stuff/xlm_roBERTa.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/Desktop/Personal/conversational_ai/test_stuff/xlm_roBERTa.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/Desktop/Personal/conversational_ai/test_stuff/xlm_roBERTa.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     no_cuda\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/Desktop/Personal/conversational_ai/test_stuff/xlm_roBERTa.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/Desktop/Personal/conversational_ai/test_stuff/xlm_roBERTa.ipynb#W4sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/Desktop/Personal/conversational_ai/test_stuff/xlm_roBERTa.ipynb#W4sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/Desktop/Personal/conversational_ai/test_stuff/xlm_roBERTa.ipynb#W4sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/Desktop/Personal/conversational_ai/test_stuff/xlm_roBERTa.ipynb#W4sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mdev_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/Desktop/Personal/conversational_ai/test_stuff/xlm_roBERTa.ipynb#W4sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nicol/Desktop/Personal/conversational_ai/test_stuff/xlm_roBERTa.ipynb#W4sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/Desktop/Personal/conversational_ai/test_stuff/xlm_roBERTa.ipynb#W4sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m test_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mevaluate(test_dataset)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/Desktop/Personal/conversational_ai/test_stuff/xlm_roBERTa.ipynb#W4sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m accuracies\u001b[39m.\u001b[39mappend(test_results[\u001b[39m\"\u001b[39m\u001b[39meval_accuracy\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1498\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1500\u001b[0m )\n\u001b[1;32m-> 1501\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1502\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1503\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1504\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1505\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1506\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1816\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1814\u001b[0m     optimizer_was_run \u001b[39m=\u001b[39m scale_before \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m scale_after\n\u001b[0;32m   1815\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1816\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m   1818\u001b[0m \u001b[39mif\u001b[39;00m optimizer_was_run \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed:\n\u001b[0;32m   1819\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:349\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    347\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    348\u001b[0m \u001b[39m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[1;32m--> 349\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mexp_avg\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros_like(p\u001b[39m.\u001b[39;49mdata)\n\u001b[0;32m    350\u001b[0m \u001b[39m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m    351\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mexp_avg_sq\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(p\u001b[39m.\u001b[39mdata)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 8.00 GiB total capacity; 7.20 GiB already allocated; 0 bytes free; 7.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "\n",
    "model_id = 'xlm-roberta-large'\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "    \n",
    "print(f\"*** {model_id} ***\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(label_names))\n",
    "\n",
    "train_texts_encoded = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "dev_texts_encoded = tokenizer(dev_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_texts_encoded = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = ClassificationDataset(train_texts_encoded, train_labels)\n",
    "dev_dataset = ClassificationDataset(dev_texts_encoded, dev_labels)\n",
    "test_dataset = ClassificationDataset(test_texts_encoded, test_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    warmup_steps=int(len(train_dataset)/16),\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    save_total_limit=10,\n",
    "    load_best_model_at_end=True,\n",
    "    no_cuda=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "accuracies.append(test_results[\"eval_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"max_split_size_mb=500\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.cuda' has no attribute 'get_allocator_stats'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nicol\\Desktop\\Personal\\conversational_ai\\test_stuff\\xlm_roBERTa.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nicol/Desktop/Personal/conversational_ai/test_stuff/xlm_roBERTa.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nicol/Desktop/Personal/conversational_ai/test_stuff/xlm_roBERTa.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mget_allocator_stats())\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.cuda' has no attribute 'get_allocator_stats'"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "print(torch.cuda.get_allocator_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.memory_allocated()/1048576 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85c576d62e5a69baba7dcae6282c7bf6fba6f8d537c9cbb11ca984aece3c77a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
